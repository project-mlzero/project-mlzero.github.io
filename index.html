<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MLZero: A Multi-Agent System for End-to-end Machine Learning Automation">
  <meta name="keywords" content="MLZero, Multi-Agent System, Machine Learning Automation, AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MLZero: A Multi-Agent System for End-to-end Machine Learning Automation</title>

  <link rel="icon" href="./assets/logos/autogluon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/tool_cards.css">
  <link rel="stylesheet" href="./static/css/top_button.css">
  <link rel="stylesheet" href="./static/css/navbar.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/video.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>

  <!-- Add navbar.js script -->
  <script src="./static/js/navbar.js"></script>

  <!-- Add MathJax -->
  <script src="./static/js/mathjax.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Add visualization.js script -->
  <script src="./static/js/visualization.js"></script>
</head>


<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
  </nav>

  <!-- Add Table of Contents -->
  <div class="toc-wrapper">
    <h4 style="margin-bottom: 10px;">
      <a href="#">Table of Contents</a>
    </h4>
    <ul class="toc-list">
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#visualization">Visualization</a></li>
      <li>
        <a href="#framework">Framework</a>
        <!-- <div class="toc-subitem">
          <a href="#task-specific-tool-selection">Task-Specific Tool Selection</a>
        </div> -->
      </li>
      <li>
        <a href="#results">Results</a>
        <!-- <div class="toc-subitem">
          <a href="#main-results">Main Results</a>
          <a href="#ablation-study">Ablation Study</a>
        </div> -->
      </li>
      <li><a href="#Share">Share</a></li>
      <li><a href="#BibTeX">BibTeX</a></li>
    </ul>
  </div>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <span style="vertical-align: middle">
                <span style="color: var(--color-tools-blue);">ML</span><span
                  style="color: var(--color-tools-blue);">Zero</span>
              </span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              A Multi-Agent System for End-to-end Machine Learning Automation
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=_PA_5_kAAAAJ" target="_blank">Haoyang Fang</a>
                <a href="mailto:haoyfang@amazon.com"><i class="fas fa-envelope"></i></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Prwxh24AAAAJ" target="_blank">Boran Han</a>
                <a href="mailto:boranhan@amazon.com"><i class="fas fa-envelope"></i></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=I0nj-TcAAAAJ" target="_blank">Nick Erickson</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=pIEuyR8AAAAJ" target="_blank">Xiyuan Zhang</a>,</span>
              <span class="author-block">
                <a href="https://github.com/suzhoum" target="_blank">Su Zhou</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=9isjTU8AAAAJ" target="_blank">Anirudh Dagar</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=CBmDAOEAAAAJ" target="_blank">Jiani Zhang</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ufCuFocAAAAJ" target="_blank">Ali Caner Turkmen</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Y3oqP5gAAAAJ" target="_blank">Tony Hu</a>
                <a href="mailto:tonyhu@amazon.com"><i class="fas fa-envelope"></i></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=yWJ9BqEAAAAJ" target="_blank">Huzefa Rangwala</a>
                <a href="mailto:rhuzefa@amazon.com"><i class="fas fa-envelope"></i></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7k_1QFIAAAAJ" target="_blank">Ying Nian Wu</a>
                <a href="mailto:wunyin@amazon.com"><i class="fas fa-envelope"></i></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=IKUm624AAAAJ" target="_blank">Bernie Wang</a>
                <a href="mailto:yuyawang@amazon.com"><i class="fas fa-envelope"></i></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ElqwScwAAAAJ" target="_blank">George Karypis</a>
                <a href="mailto:gkarypis@amazon.com"><i class="fas fa-envelope"></i></a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">AWS</span><br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2502.11271.pdf"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <!-- arxiv link -->
                <span class="link-block">
                  <a href="https://arxiv.org" class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/autogluon/autogluon-assistant"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- PyPI Link. -->
                <span class="link-block">
                  <a href="https://pypi.org/project/autogluon.assistant/"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <p style="font-size:18px">ðŸ“¦</p>
                    </span>
                    <span>PyPI</span>
                  </a>
                </span>
                <!-- Visualization Link. -->
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="assets/images/aga-abstract.jpg" width="100%">
        <p style="text-align: left;">
          MLZero: An end-to-end multi-agent system that integrates specialized perception agents 
          with dual memory modules (semantic and episodic) to power iterative coding cycles, 
          transforming raw data into ready-to-use models and prediction outputs with zero human intervention.

          The overall framework of MLZero.
          (1) \textbf{Perception} that interprets arbitrary data inputs and transforms them into structured context; 
          (2) \textbf{Semantic Memory} that enriches the system with knowledge of the ML Library; 
          (3) \textbf{Episodic Memory} that maintains chronological execution records for targeted debugging; 
          and (4) \textbf{Iterative Coding} that implements a refinement process with feedback loops and augmented memory. 
        </p>
      </div>
    </div>
  </section>

  <!-- Introduction -->
  <section class="section" id="introduction">
    <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">Introduction</h2>
    <div class="content has-text-justified">
    <p>
    Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce <b>MLZero</b>, a novel multi-agent framework powered by <b>Large Language Models (LLMs)</b> that enables end-to-end ML automation across diverse data modalities with minimal human intervention.
    </p>
    <p>
    <b>MLZero</b> employs a cognitive perception module that transforms raw multimodal inputs into perceptual context, addressing key LLM limitations through semantic and episodic memory. Our system demonstrates superior performance on MLE-Bench Lite, securing <b>six gold medals</b> and outperforming competitors on our Multimodal AutoML Agent Benchmark with a success rate of <b>0.92 (+263.6%)</b>. <b>MLZero</b> maintains robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.
    </p>
    </div>
    <div class="content has-text-justified">
    <p>
    Our approach integrates specialized perception agents with dual memory modules for iterative code development and error correction. <b>MLZero</b> not only overcomes limitations of previous LLM-based approaches but also represents a truly end-to-end system with superior performance across diverse machine learning tasks. Our main contributions include:
    </p>
    <ul style="text-align: left; margin-left: 2em;">
      <li>A novel multi-agent system delivering high-quality end-to-end multimodal ML solutions with minimal human intervention</li>
      <li>Superior performance on MLE-Bench Lite with more medal counts (6 gold) and higher success rate</li>
      <li>A comprehensive benchmark suite evaluating challenging scenarios including multilingual, multitable, and zero-shot tasks</li>
      <li>Empirical evidence showing <b>MLZero</b> outperforms existing ML agents across all metrics (<b>+263.6%</b> success rate)</li>
      <li>Detailed ablation studies identifying key components driving performance gains</li>
    </ul>
    </div>
    <div class="container is-max-desktop">
    <div class="content has-text-centered">
    <img src="assets/images/mlzero_table_1.png" width="80%">
    </div>
    </div>
    </div>
    </div>
    </div>
    </section>
  <!--/ Introduction -->

  <!-- Framework -->
  <section class="section" id="framework">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The <span style="color: var(--color-ml-red);"><b>ML</b></span><span
              style="color: var(--color-zero-blue);"><b>Zero</b></span> Framework</h2>
          <div class="content has-text-justified">
            <div class="container is-max-desktop">
              <div class="content has-text-centered">
                <img src="assets/images/agent.jpg" alt="MLZero Architecture" width="80%">
              </div>
            </div>
            <p>
              We present <span style="color: var(--color-ml-red);"><b>ML</b></span><span
                style="color: var(--color-zero-blue);"><b>Zero</b></span>, a multi-agent system that automates 
              end-to-end solutions for <b>multimodal ML tasks</b>. Given input data $x$ and optional user inputs 
              $U_{\text{opt}}$, the system produces solutions including predicted outputs $y$, code 
              artifacts $C$, and execution logs $L$:
              $\mathcal{F}(x, U^{\text{opt}}) = (y, C, L)$.
            </p>
            <p>
              Our ML model building process for various tasks is achieved by generating code employing different ML 
              libraries and executing it. For supervised learning tasks, $x$ typically includes labeled training data, 
              unlabeled test data, and a brief task description or instruction. For zero-shot tasks, $x$ would simply 
              consist of unlabeled test data and the task description. Through this comprehensive system, <span 
              style="color: var(--color-ml-red);"><b>ML</b></span><span
              style="color: var(--color-zero-blue);"><b>Zero</b></span> effectively bridges the gap between noisy raw 
              data inputs and sophisticated ML solutions, providing a truly end-to-end automated ML framework adaptive 
              to any modalities.
            </p>
            <p>
              Our system comprises four modules, where each module is a subsystem with one or more agents, and each 
              agent is a specialized LLM augmented with utility functions: (1) <b>Perception</b> that interprets 
              arbitrary data inputs and transforms them into structured context; (2) <b>Semantic Memory</b> that 
              enriches the system with knowledge of the ML Library; (3) <b>Episodic Memory</b> that maintains 
              chronological execution records for targeted debugging; and (4) <b>Iterative Coding</b> that implements 
              a refinement process with feedback loops and augmented memory.
            </p>
          </div>
          
          <h3 class="title is-4">Perception Module</h3>
          <div class="content has-text-justified">
            <p>
              The Perception module $\mathcal{P}$ acts as the cognitive lens of the system, orchestrating the 
              transformation of various data inputs into actionable ML workflow specifications:
              $\mathcal{P}(x, U^{\text{opt}}) = (P, M)$.
              This module consists of three agents: the <b>File grouping and file perception agent</b> performs structural 
              analysis of raw data $x$, grouping similar files and interpreting file contents; the <b>Task perception agent</b> 
              extracts semantic information from raw data, derived context, and user input $U_{\text{opt}}$ to identify 
              objectives, constraints, and evaluation criteria; and the <b>ML Library selection agent</b> employs context-aware 
              reasoning to match problem characteristics with the appropriate ML Library $M$.
            </p>
          </div>
          
          <h3 class="title is-4">Semantic Memory Module</h3>
          <div class="content has-text-justified">
            <p>
              The Semantic Memory Module $\mathcal{S}_t$ enhances the LLM's parametric knowledge with domain-specific 
              information from external knowledge bases at each iteration $t$. These knowledge bases are constructed 
              offline by two agents: the <b>summarization agent</b> compresses relevant knowledge into concise paragraphs 
              serving as queryable indices, while the <b>condensation agent</b> transforms this knowledge into precise 
              and streamlined guidance. At each iteration $t$, given the error context $R_t$, the Semantic Memory Module 
              processes this information through its <b>retrieval agent</b> to query the knowledge base of the selected 
              ML library $M$, extracting condensed information $G_t$:
              $\mathcal{S}_t(P, M, R_t) = G_t$.
            </p>
          </div>
          
          <h3 class="title is-4">Episodic Memory Module</h3>
          <div class="content has-text-justified">
            <p>
              The Episodic Memory module, $\mathcal{E}_t$, enhances the success rate of <span 
              style="color: var(--color-ml-red);"><b>ML</b></span><span
              style="color: var(--color-zero-blue);"><b>Zero</b></span> in ML model building by providing error 
              context $R_t$ at each iteration $t$ leveraging its chronological record of the system execution history:
              $\mathcal{E}_t(P, C_{t-1}, L_{t-1}, G_{t-1}, R_{t-1}) = R_t$.
              This component is initialized with the perception context $P$ and progressively stores the interaction data 
              at each iteration. When invoked during code generation, the <b>error analyzer agent</b> distills encountered 
              issues and contexts into concise error summaries paired with fix suggestions, enabling subsequent coding agents 
              to efficiently address specific problems without processing excessive contextual information.
            </p>
          </div>
          
          <h3 class="title is-4">Iterative Coding Module</h3>
          <div class="content has-text-justified">
            <p>
              With the support of components above, our system enters an iterative coding process $\mathcal{G}_t$, where 
              at each iteration $t$ it refines the solution based on execution feedback:
              $\mathcal{G}_t(P, U^{\text{opt}}_t, R_t, G_t) = (y_t, C_t, L_t)$.
              For each iteration $t$, the system first combines the perceptual context $P$, optional user input 
              $U^{\text{opt}}_t$, error context $R_t$, and the retrieved knowledge $G_t$ to guide the <b>coder agent</b> 
              in producing executable code $C_t$. The system then executes the generated code in a configured environment, 
              capturing logs $L_t$ and stores the model output $y_t$. The <b>executer agent</b> analyzes these results and 
              logs to determine the next steps: finalizing output upon success or identifying errors and initiating the next 
              coding iteration.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Framework -->


  <section class="section" id="results">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>
          <h3 class="title is-4">Main Results: Comprehensive Evaluation</h3>
          <div class="content has-text-justified">
            <p>
              To evaluate the effectiveness of <span style="color: var(--color-ml-blue);"><b>ML</b></span><span
                style="color: var(--color-zero-green);"><b>Zero</b></span>, we conducted extensive experiments across multiple benchmarks and datasets. Our evaluations span <b>two primary benchmarks</b>: MLE-bench Lite with <b>21 diverse Kaggle competitions</b> and the Multimodal AutoML Agent Benchmark with <b>25 diverse datasets</b> covering various modalities and ML tasks.
            </p>
  
            <div id="results-carousel" class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/images/mlebench_vis.jpg" alt="MLEbench Results" width="60%">
                  <p>Performance comparison on MLEbench-Lite. <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> demonstrates superior performance with an average rank of 1.43, significantly outperforming competing approaches including AIDE (2.36), ResearchAgent (MLAB) (3.29), and CodeActAgent (OpenHands) (2.93). <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> achieves the highest success rate of 86% on MLEbench-Lite, compared to AIDE (81%), MLAB (62%), and OpenHands (71%), while also outperforming competitors in medal counts with six gold and two silver medals.
                  </p>
                </div>
              </div>
  
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/images/mlzero_table_1.png" alt="Main Results Table" width="90%">
                  <p>Performance comparison between <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> and several state-of-the-art machine learning and coding agents on our Multimodal AutoML Agent Benchmark. <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> with <b>def</b>ault configuration achieves a markedly higher success rate (92.0%) compared to all competing agents and delivers solutions of superior quality, as evidenced by its substantially lower average rank (2.42). The <b>8B</b> configuration, which uses LLama 3.1 8B, maintains better performance (45.3% success rate, 5.14 rank average) than other agents despite a significantly reduced model size. In the 24-hour extended run-time experiment (<b>24hrs</b>), <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> approaches expert-level performance given sufficient computational resources.
                  </p>
                </div>
              </div>
            </div>
          </div>
          
          <h3 class="title is-4">Implementation Details & Ablation Study</h3>
          <div class="content has-text-justified">
            <p>
              Each agent was assigned a 3-hour time limit per dataset to produce results. It is important to note that only <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> and Codex CLI operate truly end-to-end, while other agents require varying degrees of preprocessing or postprocessing to function on our benchmark. For example, DS-Agent requires manual code execution, while results from AIDE and AutoKaggle needed manual extraction from working directories.
            </p>
            <p>
              To assess our method without the advantage of its integrated external knowledge, the <b>-ext</b> configuration removes all access to external ML libraries. This configuration yields a 69.3% success rate and a 4.94 average rank, still outperforming all competitors and highlighting the efficiency of <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span>'s other components. We also investigated the contribution of episodic memory through the <b>-epi</b> configuration, which removes episodic memory but retains the LLM's conversation history within the coder agent. This setup achieves an 86.7% success rate with a 2.86 average rank, demonstrating that while episodic memory provides significant benefits, maintaining a coherent conversational context still yields reasonable performance.
            </p>
            <p>
              In contrast, we explored modifying AIDE to access external knowledge, indicated as <b>+ext</b>. While it shows improvement, <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> continued to outperform it under these comparable conditions. This result underscores that the superior performance of <span style="color: var(--color-ml-blue);"><b>ML</b></span><span style="color: var(--color-zero-green);"><b>Zero</b></span> stems from its overall system design, not merely from the inclusion of episodic memory or external knowledge in isolation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@misc{fang2025mlzeromultiagentendtoendmachine,
        title={MLZero: A Multi-Agent System for End-to-end Machine Learning Automation}, 
        author={Haoyang Fang and Boran Han and Nick Erickson and Xiyuan Zhang and Su Zhou and Anirudh Dagar and Jiani Zhang and Ali Caner Turkmen and Cuixiong Hu and Huzefa Rangwala and Ying Nian Wu and Bernie Wang and George Karypis},
        year={2025},
        eprint={2505.13941},
        archivePrefix={arXiv},
        primaryClass={cs.MA},
        url={https://arxiv.org/abs/2505.13941}, 
  }</code></pre>
    </div>
  </section>

  <section>
    <div class="section" id="org-banners" style="display:flex">
      <a href="https://aws.amazon.com/" target="_blank" rel="external">
        <img class="center-block org-banner" src="assets/logos/amazon.png">
      </a>
      <a href="https://github.com/autogluon/autogluon" target="_blank" rel="external">
        <img class="center-block org-banner" src="assets/logos/autogluon.png">
      </a>
    </div>
  </section>


  <!-- License -->
  <footer class="footer">
    <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://octotools.github.io/">OctoTools</a>, licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </footer>

</body>

  <!-- Back to top button -->
  <button onclick="topFunction()" id="topButton" title="Go to top">
    <i class="fas fa-arrow-up"></i>
  </button>


  
</html>